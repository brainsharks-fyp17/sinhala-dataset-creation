{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9CgXQb6ZWH9"
      },
      "source": [
        "Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4LsVCET1P6h",
        "outputId": "83740669-b9d7-4198-f090-f46c0dd99744"
      },
      "source": [
        "!git clone https://github.com/brainsharks-fyp17/sinhala-dataset-creation.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sinhala-dataset-creation'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 20 (delta 5), reused 15 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5d3OwGiZZwj"
      },
      "source": [
        "Install dependencies using pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc0tgwN4NoHx",
        "outputId": "8c4a1ab2-d8d5-4410-c31c-5c1ccafb30f9"
      },
      "source": [
        "!cd /content/sinhala-dataset-creation/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4zV-ZFhZfNu"
      },
      "source": [
        "Mount google drive to get datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44EugIH31zPw",
        "outputId": "0dfd762d-8afe-4ec7-b6fb-95e62fb8f6bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2pjSMk7ZjgE"
      },
      "source": [
        "Remove sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7shb8PNAKUN8"
      },
      "source": [
        "!rm /content/sinhala-dataset-creation/datasets/raw/sample0.txt\n",
        "!rm /content/sinhala-dataset-creation/datasets/raw/sample1.txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXjqFg9dZp4V"
      },
      "source": [
        "Copy OSCAR filtered data from the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja4dDldr10oL"
      },
      "source": [
        "cp /content/drive/Shareddrives/Project-\\ Rephrasing\\ Legal\\ Documents\\ in\\ Layman\\ Terms/Datasets/OSCAR/si_dedup.txt /content/sinhala-dataset-creation/datasets/raw/si_dedup.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwAIahHuZzLX"
      },
      "source": [
        "Copy News data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I7hZXRW19Ka"
      },
      "source": [
        "cp /content/drive/Shareddrives/Project-\\ Rephrasing\\ Legal\\ Documents\\ in\\ Layman\\ Terms/Datasets/RAW\\ news\\ datasets/Raw-news-sinhala-extracted.tar.xz /content/sinhala-dataset-creation/datasets/raw/"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6lCz_OZ4eG"
      },
      "source": [
        "Extract news data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl-K22OO2BNZ",
        "outputId": "f246c914-376e-4569-f62c-384ef311545e"
      },
      "source": [
        "!tar -xvf /content/sinhala-dataset-creation/datasets/raw/Raw-news-sinhala-extracted.tar.xz -C /content/sinhala-dataset-creation/datasets/raw/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw-news-sinhala.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJywmEEoZ-EE"
      },
      "source": [
        "Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ8EdAJa2CGq"
      },
      "source": [
        "!python pipeline.py"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4IHdnXRaF4t"
      },
      "source": [
        "Create a zip file from the tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lSCoc5T5cPX",
        "outputId": "0e0dc50f-be93-48fc-a75f-660b8e0ccf6d"
      },
      "source": [
        "!zip -r /content/tokenized.zip /content/sinhala-dataset-creation/datasets/tokenized"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/ (stored 0%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4200000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4700000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3900000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3300000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3000000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1400000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1900000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_100000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3800000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1200000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_6100000.txt (deflated 82%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5000000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_6200000.txt (deflated 82%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2300000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2800000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1000000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_300000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_900000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_700000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1800000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5900000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4000000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1300000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4400000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5800000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2700000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4800000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4500000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2900000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2500000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2200000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3600000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1500000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2000000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1600000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1700000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4600000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2400000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_6300000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4300000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_400000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5500000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3400000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5400000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3500000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3700000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_6000000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5600000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_600000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5300000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_500000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4900000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_200000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5100000.txt (deflated 80%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_1100000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2100000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5200000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_4100000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3200000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_5700000.txt (deflated 81%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_3100000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_2600000.txt (deflated 79%)\n",
            "  adding: content/sinhala-dataset-creation/datasets/tokenized/tokenized_shard_800000.txt (deflated 79%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}